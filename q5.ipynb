{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ad8640",
   "metadata": {},
   "source": [
    "# Query 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a6b89c",
   "metadata": {},
   "source": [
    "## Import and describe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0776db59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "\n",
    "EXECUTORS = \"8\"\n",
    "CORES = \"1\"\n",
    "MEMORY = \"2g\"\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query5\") \\\n",
    "    .config(\"spark.executor.instances\", EXECUTORS) \\\n",
    "    .config(\"spark.executor.cores\", CORES) \\\n",
    "    .config(\"spark.executor.memory\", MEMORY) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load and filter Crime Data\n",
    "crime_data_path_1 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "crime_data_path_2 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "crime_data_1 = spark.read.csv(crime_data_path_1, header=True, inferSchema=True) \\\n",
    "    .withColumnRenamed('AREA ', 'AREA')\n",
    "crime_data_2 = spark.read.csv(crime_data_path_2, header=True, inferSchema=True)\n",
    "crime_data = (\n",
    "    crime_data_1.union(crime_data_2)\n",
    "    # Drop null island (0,0) entries\n",
    "    .filter((col('LON') != 0) | (col('LAT') != 0))\n",
    "    # Keep only selected columns\n",
    "#     .select('DR_NO', 'DATE OCC', 'AREA', 'AREA NAME', 'Status', 'Status Desc')\n",
    ")\n",
    "\n",
    "# Load LA Police Stations data\n",
    "laps_data_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\"\n",
    "laps_data = spark.read.csv(laps_data_path , header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf338d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Crime Data:\")\n",
    "crime_data.printSchema()\n",
    "\n",
    "print(\"\\nLA Police Stations Data:\")\n",
    "laps_data.printSchema()\n",
    "\n",
    "laps_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c6d36",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9876ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.register.geo_registrator import SedonaRegistrator\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def runQuery():\n",
    "    # Register Sedona\n",
    "    SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load and filter Crime Data\n",
    "    crime_data_path_1 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "    crime_data_path_2 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "    crime_data_1 = spark.read.csv(crime_data_path_1, header=True, inferSchema=True) \\\n",
    "        .withColumnRenamed('AREA ', 'AREA')\n",
    "    crime_data_2 = spark.read.csv(crime_data_path_2, header=True, inferSchema=True)\n",
    "    crime_data = (\n",
    "        crime_data_1.union(crime_data_2)\n",
    "        .select('DR_NO','LAT', 'LON')\n",
    "        .filter((col('LON') != 0) | (col('LAT') != 0))\n",
    "        .filter(col('LAT').isNotNull() & col('LON').isNotNull())\n",
    "        .withColumn('LAT', col('LAT').cast(DoubleType()))\n",
    "        .withColumn('LON', col('LON').cast(DoubleType())) \n",
    "    )\n",
    "\n",
    "    # Load and filter LA Police Stations data\n",
    "    laps_data_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\"\n",
    "    laps_data = (\n",
    "        spark.read.csv(laps_data_path , header=True, inferSchema=True)\n",
    "        .select(\"FID\", \"X\", \"Y\", \"DIVISION\")\n",
    "        .withColumn(\"LAT\", col(\"Y\").cast(DoubleType()))\n",
    "        .withColumn(\"LON\", col(\"X\").cast(DoubleType()))\n",
    "    )\n",
    "\n",
    "    # Register spatial DataFrames\n",
    "    crime_data = crime_data.withColumn(\"geometry\", expr(\"ST_Point(LON, LAT)\"))\n",
    "    laps_data = laps_data.withColumn(\"geometry\", expr(\"ST_Point(LON, LAT)\"))\n",
    "\n",
    "    # Perform a cross join to calculate distances between all crime points and police stations\n",
    "    crossed = crime_data.alias(\"c\").join(\n",
    "        laps_data.alias(\"l\"),\n",
    "        how=\"cross\"\n",
    "    ).withColumn(\n",
    "        \"distance\",\n",
    "        expr(\"ST_Distance(c.geometry, l.geometry)\") * 111\n",
    "    )\n",
    "\n",
    "    # Find the closest station for each crime\n",
    "    crime_with_closest_station = crossed.withColumn(\n",
    "        \"min_distance\",\n",
    "        min(\"distance\").over(Window.partitionBy(\"c.LAT\", \"c.LON\"))\n",
    "    ).filter(col(\"distance\") == col(\"min_distance\"))\n",
    "\n",
    "    # Group results\n",
    "    result = crime_with_closest_station.groupBy(\"l.DIVISION\").agg(\n",
    "        count(\"*\").alias(\"crime_count\"),\n",
    "        avg(\"distance\").alias(\"mean_distance\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"crime_count\").desc()) \\\n",
    "    .withColumn(\n",
    "        \"mean_distance\",\n",
    "        round(\"mean_distance\", 3)\n",
    "    )\n",
    "\n",
    "    # Show the result\n",
    "    result.show()\n",
    "\n",
    "    # Stop timing and print out the execution duration\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    # Save Results\n",
    "    output_path = \"s3://groups-bucket-dblab-905418150721/group7/q5_results\"\n",
    "    result.write \\\n",
    "        .option(\"header\", True) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(f\"{output_path}/results\")\n",
    "    \n",
    "    return elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fef78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_time = 0\n",
    "num_exp = 10\n",
    "for i in range(num_exp):\n",
    "    print(\"Working on experiment no \", i)\n",
    "    proc_time += runQuery()/num_exp\n",
    "    \n",
    "print(f\"Average Processing Time: {proc_time:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
