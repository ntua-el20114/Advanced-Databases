{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "188a665a",
   "metadata": {},
   "source": [
    "# Load Datasets and Check Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fe8f028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3302</td><td>application_1732639283265_3258</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3258/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-119.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3258_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of GeoJSON data:\n",
      "root\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: struct (nullable = true)\n",
      " |    |-- BG10: string (nullable = true)\n",
      " |    |-- BG10FIP10: string (nullable = true)\n",
      " |    |-- BG12: string (nullable = true)\n",
      " |    |-- CB10: string (nullable = true)\n",
      " |    |-- CEN_FIP13: string (nullable = true)\n",
      " |    |-- CITY: string (nullable = true)\n",
      " |    |-- CITYCOM: string (nullable = true)\n",
      " |    |-- COMM: string (nullable = true)\n",
      " |    |-- CT10: string (nullable = true)\n",
      " |    |-- CT12: string (nullable = true)\n",
      " |    |-- CTCB10: string (nullable = true)\n",
      " |    |-- HD_2012: long (nullable = true)\n",
      " |    |-- HD_NAME: string (nullable = true)\n",
      " |    |-- HOUSING10: long (nullable = true)\n",
      " |    |-- LA_FIP10: string (nullable = true)\n",
      " |    |-- OBJECTID: long (nullable = true)\n",
      " |    |-- POP_2010: long (nullable = true)\n",
      " |    |-- PUMA10: string (nullable = true)\n",
      " |    |-- SPA_2012: long (nullable = true)\n",
      " |    |-- SPA_NAME: string (nullable = true)\n",
      " |    |-- SUP_DIST: string (nullable = true)\n",
      " |    |-- SUP_LABEL: string (nullable = true)\n",
      " |    |-- ShapeSTArea: double (nullable = true)\n",
      " |    |-- ShapeSTLength: double (nullable = true)\n",
      " |    |-- ZCTA10: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n",
      "Dataset loading time: 17.8916 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col, sum as spark_sum, regexp_replace, round, count\n",
    "import time\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query3\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"2\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"10\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "    .config(\"spark.eventLog.dir\", \"s3://groups-bucket-dblab-905418150721/group7/logs/\") \\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Load GeoJSON file (2010 Census Blocks)\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "    .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "    .selectExpr(\"explode(features) as features\") \\\n",
    "    .select(\"features.*\")\n",
    "\n",
    "# Check geojson datatypes\n",
    "print(\"Schema of GeoJSON data:\")\n",
    "blocks_df.printSchema()\n",
    "\n",
    "# Flatten GeoJSON properties\n",
    "flattened_df = blocks_df.select(\n",
    "    [col(f\"properties.{col_name}\").alias(col_name) for col_name in\n",
    "     blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]\n",
    ").drop(\"properties\").drop(\"type\")\n",
    "\n",
    "# Load dataset 2 with median income\n",
    "income_data_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\"\n",
    "income_df = spark.read.csv(income_data_path, header=True, inferSchema=True).select(\"Zip Code\", \"Estimated Median Income\")\n",
    "\n",
    "# Load crime datasets\n",
    "crime_data_path_1 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "crime_data_path_2 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "\n",
    "# Read both datasets\n",
    "data_1 = spark.read.csv(crime_data_path_1, header=True, inferSchema=True)\n",
    "data_2 = spark.read.csv(crime_data_path_2, header=True, inferSchema=True)\n",
    "\n",
    "# Calculate dataset preparation time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Dataset loading time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef449b3a",
   "metadata": {},
   "source": [
    "# Calculate median income per person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13af114d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution plan for Census - Income Join:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [ZCTA10#66, COMM#49, Total Housing#354L, Total Population#356L, Estimated Median Income#190]\n",
      "   +- BroadcastHashJoin [cast(ZCTA10#66 as int)], [Zip Code#188], LeftOuter, BuildRight, false\n",
      "      :- HashAggregate(keys=[ZCTA10#66, COMM#49], functions=[sum(HOUSING10#55L), sum(POP_2010#58L)], schema specialized)\n",
      "      :  +- Exchange hashpartitioning(ZCTA10#66, COMM#49, 200), ENSURE_REQUIREMENTS, [plan_id=149]\n",
      "      :     +- HashAggregate(keys=[ZCTA10#66, COMM#49], functions=[partial_sum(HOUSING10#55L), partial_sum(POP_2010#58L)], schema specialized)\n",
      "      :        +- Project [features#33.properties.ZCTA10 AS ZCTA10#66, features#33.properties.HOUSING10 AS HOUSING10#55L, features#33.properties.POP_2010 AS POP_2010#58L, features#33.properties.COMM AS COMM#49]\n",
      "      :           +- Filter ((isnotnull(features#33.properties.CITY) AND isnotnull(features#33.properties.ZCTA10)) AND ((features#33.properties.CITY = Los Angeles) AND (NOT (features#33.properties.ZCTA10 =  ) AND isnotnull(features#33.properties.COMM))))\n",
      "      :              +- Generate explode(features#25), false, [features#33]\n",
      "      :                 +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "      :                    +- FileScan geojson [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=152]\n",
      "         +- Exchange hashpartitioning(Zip Code#188, 200), REPARTITION_BY_NUM, [plan_id=145]\n",
      "            +- Filter isnotnull(Zip Code#188)\n",
      "               +- FileScan csv [Zip Code#188,Estimated Median Income#190] Batched: false, DataFilters: [isnotnull(Zip Code#188)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "\n",
      "\n",
      "+---------------+-------------+----------------+------------------------+\n",
      "|COMM           |Total Income |Total Population|Median Income Per Person|\n",
      "+---------------+-------------+----------------+------------------------+\n",
      "|Miracle Mile   |6.23571124E8 |16057           |38834.84611             |\n",
      "|Harvard Heights|2.00373929E8 |16951           |11820.77335             |\n",
      "|Mar Vista      |1.297050954E9|39126           |33150.61478             |\n",
      "|West Hills     |1.102506327E9|38333           |28761.28472             |\n",
      "|Hollywood      |1.600746144E9|62412           |25648.05076             |\n",
      "|Glassell Park  |5.59307684E8 |29712           |18824.30277             |\n",
      "|Hancock Park   |3.35079009E8 |15557           |21538.7934              |\n",
      "|Silverlake     |1.029245174E9|41305           |24918.17393             |\n",
      "|Reseda         |1.215749753E9|71819           |16927.96827             |\n",
      "|Park La Brea   |4.31455662E8 |11782           |36619.90002             |\n",
      "|Del Rey        |9.29911374E8 |27614           |33675.35938             |\n",
      "|Playa Del Rey  |1.4376036E8  |3158            |45522.59658             |\n",
      "|Baldwin Hills  |4.95497352E8 |28637           |17302.69763             |\n",
      "|Shadow Hills   |1.1228387E8  |4455            |25204.01122             |\n",
      "|Sunland        |5.13225685E8 |19648           |26121.0141              |\n",
      "|Elysian Valley |1.58476661E8 |9750            |16254.01651             |\n",
      "|Toluca Terrace |2.6237959E7  |1301            |20167.5319              |\n",
      "|Elysian Park   |7.3060257E7  |5267            |13871.32276             |\n",
      "|Eagle Rock     |7.79488004E8 |37683           |20685.40201             |\n",
      "|Vermont Knolls |1.75107223E8 |16678           |10499.29386             |\n",
      "+---------------+-------------+----------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Median Income per person time: 32.9399 seconds"
     ]
    }
   ],
   "source": [
    "# Clean Census Dataset\n",
    "la_census_df = flattened_df.filter(col(\"CITY\") == \"Los Angeles\").select(\"ZCTA10\", \"HOUSING10\", \"POP_2010\", \"COMM\")\n",
    "la_census_cleaned = la_census_df.filter(col(\"ZCTA10\") != \" \")\n",
    "la_census_cleaned = la_census_cleaned.filter(col(\"COMM\").isNotNull())\n",
    "\n",
    "# Group by ZCTA10 and COMM and aggregate Housing and Population\n",
    "merged_df = la_census_cleaned.groupBy(\"ZCTA10\", \"COMM\").agg(\n",
    "    spark_sum(\"HOUSING10\").alias(\"Total Housing\"),\n",
    "    spark_sum(\"POP_2010\").alias(\"Total Population\"))\n",
    "\n",
    "# Partition dataset to avoid shuffling\n",
    "income_df_cleaned = income_df.repartition(200, col(\"Zip Code\"))\n",
    "\n",
    "# Join census dataset with income\n",
    "merged_df_with_income = merged_df.hint(\"BROADCAST\").join(\n",
    "    income_df_cleaned, \n",
    "    merged_df[\"ZCTA10\"] == income_df_cleaned[\"Zip Code\"], \n",
    "    \"left\").drop(\"Zip Code\")\n",
    "\n",
    "# Show the execution plan\n",
    "print(\"Execution plan for Census - Income Join:\")\n",
    "merged_df_with_income.explain()\n",
    "\n",
    "# Cleanup Estimated Median Income: Removes $, commas, and converts to double\n",
    "merged_df_with_income = merged_df_with_income.withColumn(\n",
    "    \"Total Income\",\n",
    "    regexp_replace(col(\"Estimated Median Income\"), r\"[$,]\", \"\").cast(\"double\") * col(\"Total Housing\"))\n",
    "\n",
    "# Group by COMM and calculate total income and population\n",
    "final_result = merged_df_with_income.groupBy(\"COMM\").agg(\n",
    "    spark_sum(\"Total Income\").alias(\"Total Income\"),\n",
    "    spark_sum(\"Total Population\").alias(\"Total Population\"))\n",
    "\n",
    "# Calculate median income per person for each COMM\n",
    "final_result = final_result.withColumn(\"Median Income Per Person\", col(\"Total Income\") / col(\"Total Population\"))\n",
    "\n",
    "# Round the values to 5 decimals\n",
    "final_result = final_result.withColumn(\n",
    "    \"Total Income\", round(col(\"Total Income\"), 5)\n",
    "    ).withColumn(\"Median Income Per Person\", round(col(\"Median Income Per Person\"), 5))\n",
    "\n",
    "# Show results\n",
    "final_result.show(truncate=False)\n",
    "\n",
    "# Print half query timer\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Median Income per person time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9296c44",
   "metadata": {},
   "source": [
    "## Save results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60652833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of Final Result:\n",
      "root\n",
      " |-- COMM: string (nullable = true)\n",
      " |-- Total Income: double (nullable = true)\n",
      " |-- Total Population: long (nullable = true)\n",
      " |-- Median Income Per Person: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "# Έλεγχος τύπων αποτελέσματος\n",
    "print(\"Schema of Final Result:\")\n",
    "final_result.printSchema()\n",
    "\n",
    "# Αποθήκευση των αποτελεσμάτων σε αρχείο CSV\n",
    "output_path = \"s3://groups-bucket-dblab-905418150721/group7/q3_results\"  # Replace with your desired path\n",
    "final_result.write \\\n",
    "    .option(\"header\", True) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(f\"{output_path}/median_income_per_person\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d05f2f",
   "metadata": {},
   "source": [
    "# Calculate Crimes per person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95c1f854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution plan for Crimes - Census Join:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- RangeJoin geom#698: geometry, geometry#540: geometry, WITHIN\n",
      "   :- Union\n",
      "   :  :- Project [DR_NO#215 AS DR_NO#556, Date Rptd#216 AS Date Rptd#557, DATE OCC#217 AS DATE OCC#558, TIME OCC#218 AS TIME OCC#559, AREA #219 AS AREA#560, AREA NAME#220 AS AREA NAME#561, Rpt Dist No#221 AS Rpt Dist No#562, Part 1-2#222 AS Part 1-2#563, Crm Cd#223 AS Crm Cd#564, Crm Cd Desc#224 AS Crm Cd Desc#565, Mocodes#225 AS Mocodes#566, Vict Age#226 AS Vict Age#567, Vict Sex#227 AS Vict Sex#568, Vict Descent#228 AS Vict Descent#569, Premis Cd#229 AS Premis Cd#570, Premis Desc#230 AS Premis Desc#571, Weapon Used Cd#231 AS Weapon Used Cd#572, Weapon Desc#232 AS Weapon Desc#573, Status#233 AS Status#574, Status Desc#234 AS Status Desc#575, Crm Cd 1#235 AS Crm Cd 1#576, Crm Cd 2#236 AS Crm Cd 2#577, Crm Cd 3#237 AS Crm Cd 3#578, Crm Cd 4#238 AS Crm Cd 4#579, ... 5 more fields]\n",
      "   :  :  +- Filter (atleastnnonnulls(2, LON#242, LAT#241) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "   :  :     +- FileScan csv [DR_NO#215,Date Rptd#216,DATE OCC#217,TIME OCC#218,AREA #219,AREA NAME#220,Rpt Dist No#221,Part 1-2#222,Crm Cd#223,Crm Cd Desc#224,Mocodes#225,Vict Age#226,Vict Sex#227,Vict Descent#228,Premis Cd#229,Premis Desc#230,Weapon Used Cd#231,Weapon Desc#232,Status#233,Status Desc#234,Crm Cd 1#235,Crm Cd 2#236,Crm Cd 3#237,Crm Cd 4#238,... 4 more fields] Batched: false, DataFilters: [atleastnnonnulls(2, LON#242, LAT#241), isnotnull( **org.apache.spark.sql.sedona_sql.expressions...., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA :int,AREA NAME:string,Rpt Dis...\n",
      "   :  +- Project [DR_NO#289, Date Rptd#290, DATE OCC#291, TIME OCC#292, AREA#293, AREA NAME#294, Rpt Dist No#295, Part 1-2#296, Crm Cd#297, Crm Cd Desc#298, Mocodes#299, Vict Age#300, Vict Sex#301, Vict Descent#302, Premis Cd#303, Premis Desc#304, Weapon Used Cd#305, Weapon Desc#306, Status#307, Status Desc#308, Crm Cd 1#309, Crm Cd 2#310, Crm Cd 3#311, Crm Cd 4#312, ... 5 more fields]\n",
      "   :     +- Filter (atleastnnonnulls(2, LON#316, LAT#315) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "   :        +- FileScan csv [DR_NO#289,Date Rptd#290,DATE OCC#291,TIME OCC#292,AREA#293,AREA NAME#294,Rpt Dist No#295,Part 1-2#296,Crm Cd#297,Crm Cd Desc#298,Mocodes#299,Vict Age#300,Vict Sex#301,Vict Descent#302,Premis Cd#303,Premis Desc#304,Weapon Used Cd#305,Weapon Desc#306,Status#307,Status Desc#308,Crm Cd 1#309,Crm Cd 2#310,Crm Cd 3#311,Crm Cd 4#312,... 4 more fields] Batched: false, DataFilters: [atleastnnonnulls(2, LON#316, LAT#315), isnotnull( **org.apache.spark.sql.sedona_sql.expressions...., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA:int,AREA NAME:string,Rpt Dist...\n",
      "   +- Filter isnotnull(geometry#540)\n",
      "      +- ObjectHashAggregate(keys=[COMM#49], functions=[sum(POP_2010#58L), st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@522483a4, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "         +- Exchange hashpartitioning(COMM#49, 200), ENSURE_REQUIREMENTS, [plan_id=989]\n",
      "            +- ObjectHashAggregate(keys=[COMM#49], functions=[partial_sum(POP_2010#58L), partial_st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@522483a4, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "               +- Project [features#33.properties.COMM AS COMM#49, features#33.properties.POP_2010 AS POP_2010#58L, features#33.geometry AS geometry#36]\n",
      "                  +- Filter (isnotnull(features#33.properties.CITY) AND (features#33.properties.CITY = Los Angeles))\n",
      "                     +- Generate explode(features#25), false, [features#33]\n",
      "                        +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "                           +- FileScan geojson [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "\n",
      "+---------------+-----------+----------------+-------------------+\n",
      "|COMM           |crime_count|total_population|crimes_per_person  |\n",
      "+---------------+-----------+----------------+-------------------+\n",
      "|Miracle Mile   |14163      |16057           |0.8820452139253908 |\n",
      "|Mar Vista      |19434      |39126           |0.49670295966876243|\n",
      "|Harvard Heights|13235      |16951           |0.780779894991446  |\n",
      "|West Hills     |18527      |38333           |0.4833172462369238 |\n",
      "|Hollywood      |96810      |62412           |1.5511440107671601 |\n",
      "|Glassell Park  |14916      |29712           |0.5020193861066236 |\n",
      "|Hancock Park   |13975      |15557           |0.8983094426946069 |\n",
      "|Silverlake     |29840      |41305           |0.7224306984626558 |\n",
      "|Reseda         |43870      |71819           |0.6108411423161002 |\n",
      "|Park La Brea   |12079      |11782           |1.025207944321847  |\n",
      "|Del Rey        |15432      |27614           |0.5588469616861013 |\n",
      "|Playa Del Rey  |2454       |3158            |0.7770740975300824 |\n",
      "|Baldwin Hills  |34194      |28637           |1.1940496560393896 |\n",
      "|Sunland        |9821       |19648           |0.49984731270358307|\n",
      "|Shadow Hills   |2599       |4455            |0.5833894500561168 |\n",
      "|Elysian Valley |5873       |9750            |0.6023589743589743 |\n",
      "|Toluca Terrace |300        |1301            |0.23059185242121444|\n",
      "|Eagle Rock     |18931      |37683           |0.5023750762943502 |\n",
      "|Elysian Park   |5714       |5267            |1.084868046326182  |\n",
      "|Century City   |10176      |11890           |0.8558452481076535 |\n",
      "+---------------+-----------+----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Both Queries time: 70.9895 seconds"
     ]
    }
   ],
   "source": [
    "#Specify LA city\n",
    "flattened_df = flattened_df.filter(col(\"CITY\") == \"Los Angeles\")\n",
    "\n",
    "# Aggregate population by COMM to ensure no duplicates\n",
    "agg_population_df = flattened_df.groupBy(\"COMM\").agg(\n",
    "    spark_sum(\"POP_2010\").alias(\"total_population\"),  # Sum population for each COMM\n",
    "    ST_Union_Aggr(\"geometry\").alias(\"geometry\"))  # Combine geometries for each COMM\n",
    "\n",
    "# Standardize column names (trim spaces)\n",
    "data_1 = data_1.select([col(c).alias(c.strip()) for c in data_1.columns])\n",
    "data_2 = data_2.select([col(c).alias(c.strip()) for c in data_2.columns])\n",
    "\n",
    "# Combine crime datasets into one\n",
    "combined_crime_df = data_1.unionByName(data_2)\n",
    "\n",
    "# Ensure geometry points from crime data, drop rows with missing coordinates\n",
    "combined_crime_df = combined_crime_df.withColumn(\n",
    "    \"geom\", ST_Point(col(\"LON\"), col(\"LAT\"))\n",
    ").dropna(subset=[\"LON\", \"LAT\"])\n",
    "\n",
    "# Spatial join to associate crimes with aggregated COMM data\n",
    "joined_df = combined_crime_df.hint(\"MERGE\").join(agg_population_df, ST_Within(combined_crime_df.geom, agg_population_df.geometry), \"inner\")\n",
    "\n",
    "# Show the execution plan\n",
    "print(\"Execution plan for Crimes - Census Join:\")\n",
    "joined_df.explain()\n",
    "\n",
    "# Aggregate crime counts per COMM\n",
    "crime_counts = joined_df.groupBy(\"COMM\").agg(count(\"DR_NO\").alias(\"crime_count\"))\n",
    "\n",
    "# Combine crime counts with aggregated population\n",
    "pop_with_crime = crime_counts.join(agg_population_df, \"COMM\", \"inner\")\n",
    "pop_with_crime = pop_with_crime.withColumn(\n",
    "    \"crimes_per_person\", col(\"crime_count\") / col(\"total_population\"))\n",
    "\n",
    "# Registering geometry column for manual WKT conversion\n",
    "pop_with_crime.createOrReplaceTempView(\"pop_with_crime\")\n",
    "\n",
    "# Exclude geometry column if WKT conversion is not possible\n",
    "final_result2 = pop_with_crime.filter(col(\"total_population\") > 0) \\\n",
    "    .select(\"COMM\", \"crime_count\", \"total_population\", \"crimes_per_person\")\n",
    "\n",
    "# Show results\n",
    "final_result2.show(truncate=False)\n",
    "    \n",
    "# Print Total Query timer\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Both Queries time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65fa78a",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd49aaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save without geometry\n",
    "output_path = \"s3://groups-bucket-dblab-905418150721/group7/q3_results\"\n",
    "final_result2.write \\\n",
    "    .option(\"header\", True) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(f\"{output_path}/crimes_per_person\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60d3e04",
   "metadata": {},
   "source": [
    "## Join Results in one Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5487c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution plan for Final Result:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#49, Total Income#413, Total Population#404L, Median Income Per Person#418, crime_count#865L, total_population#535L, crimes_per_person#914]\n",
      "   +- SortMergeJoin [COMM#49], [COMM#1024], Inner\n",
      "      :- Sort [COMM#49 ASC NULLS FIRST], false, 0\n",
      "      :  +- Project [COMM#49, round(Total Income#402, 5) AS Total Income#413, Total Population#404L, round((Total Income#402 / cast(Total Population#404L as double)), 5) AS Median Income Per Person#418]\n",
      "      :     +- HashAggregate(keys=[COMM#49], functions=[sum(Total Income#388), sum(Total Population#356L)], schema specialized)\n",
      "      :        +- Exchange hashpartitioning(COMM#49, 200), ENSURE_REQUIREMENTS, [plan_id=2679]\n",
      "      :           +- HashAggregate(keys=[COMM#49], functions=[partial_sum(Total Income#388), partial_sum(Total Population#356L)], schema specialized)\n",
      "      :              +- Project [COMM#49, Total Population#356L, (cast(regexp_replace(Estimated Median Income#190, [$,], , 1) as double) * cast(Total Housing#354L as double)) AS Total Income#388]\n",
      "      :                 +- BroadcastHashJoin [cast(ZCTA10#66 as int)], [Zip Code#188], LeftOuter, BuildRight, false\n",
      "      :                    :- HashAggregate(keys=[ZCTA10#66, COMM#49], functions=[sum(HOUSING10#55L), sum(POP_2010#58L)], schema specialized)\n",
      "      :                    :  +- Exchange hashpartitioning(ZCTA10#66, COMM#49, 200), ENSURE_REQUIREMENTS, [plan_id=2671]\n",
      "      :                    :     +- HashAggregate(keys=[ZCTA10#66, COMM#49], functions=[partial_sum(HOUSING10#55L), partial_sum(POP_2010#58L)], schema specialized)\n",
      "      :                    :        +- Project [features#33.properties.ZCTA10 AS ZCTA10#66, features#33.properties.HOUSING10 AS HOUSING10#55L, features#33.properties.POP_2010 AS POP_2010#58L, features#33.properties.COMM AS COMM#49]\n",
      "      :                    :           +- Filter ((isnotnull(features#33.properties.CITY) AND isnotnull(features#33.properties.ZCTA10)) AND ((features#33.properties.CITY = Los Angeles) AND (NOT (features#33.properties.ZCTA10 =  ) AND isnotnull(features#33.properties.COMM))))\n",
      "      :                    :              +- Generate explode(features#25), false, [features#33]\n",
      "      :                    :                 +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "      :                    :                    +- FileScan geojson [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      :                    +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2674]\n",
      "      :                       +- Exchange hashpartitioning(Zip Code#188, 200), REPARTITION_BY_NUM, [plan_id=2554]\n",
      "      :                          +- Filter isnotnull(Zip Code#188)\n",
      "      :                             +- FileScan csv [Zip Code#188,Estimated Median Income#190] Batched: false, DataFilters: [isnotnull(Zip Code#188)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "      +- Project [COMM#1024, crime_count#865L, total_population#535L, (cast(crime_count#865L as double) / cast(total_population#535L as double)) AS crimes_per_person#914]\n",
      "         +- SortMergeJoin [COMM#1024], [COMM#879], Inner\n",
      "            :- Sort [COMM#1024 ASC NULLS FIRST], false, 0\n",
      "            :  +- HashAggregate(keys=[COMM#1024], functions=[count(DR_NO#556)], schema specialized)\n",
      "            :     +- Exchange hashpartitioning(COMM#1024, 200), ENSURE_REQUIREMENTS, [plan_id=2730]\n",
      "            :        +- HashAggregate(keys=[COMM#1024], functions=[partial_count(DR_NO#556)], schema specialized)\n",
      "            :           +- Project [DR_NO#556, COMM#1024]\n",
      "            :              +- RangeJoin geom#698: geometry, geometry#540: geometry, WITHIN\n",
      "            :                 :- Union\n",
      "            :                 :  :- Project [DR_NO#215 AS DR_NO#556,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#698]\n",
      "            :                 :  :  +- Filter (atleastnnonnulls(2, LON#242, LAT#241) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "            :                 :  :     +- FileScan csv [DR_NO#215,LAT#241,LON#242] Batched: false, DataFilters: [atleastnnonnulls(2, LON#242, LAT#241), isnotnull( **org.apache.spark.sql.sedona_sql.expressions...., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:int,LAT:double,LON:double>\n",
      "            :                 :  +- Project [DR_NO#289,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#1062]\n",
      "            :                 :     +- Filter (atleastnnonnulls(2, LON#316, LAT#315) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "            :                 :        +- FileScan csv [DR_NO#289,LAT#315,LON#316] Batched: false, DataFilters: [atleastnnonnulls(2, LON#316, LAT#315), isnotnull( **org.apache.spark.sql.sedona_sql.expressions...., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:int,LAT:double,LON:double>\n",
      "            :                 +- Filter isnotnull(geometry#540)\n",
      "            :                    +- ObjectHashAggregate(keys=[COMM#1024], functions=[st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@522483a4, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "            :                       +- Exchange hashpartitioning(COMM#1024, 200), ENSURE_REQUIREMENTS, [plan_id=2724]\n",
      "            :                          +- ObjectHashAggregate(keys=[COMM#1024], functions=[partial_st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@522483a4, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "            :                             +- Project [features#33.properties.COMM AS COMM#1024, features#33.geometry AS geometry#36]\n",
      "            :                                +- Filter ((isnotnull(features#33.properties.CITY) AND (features#33.properties.CITY = Los Angeles)) AND isnotnull(features#33.properties.COMM))\n",
      "            :                                   +- Generate explode(features#1014), false, [features#33]\n",
      "            :                                      +- Filter ((size(features#1014, true) > 0) AND isnotnull(features#1014))\n",
      "            :                                         +- FileScan geojson [features#1014] Batched: false, DataFilters: [(size(features#1014, true) > 0), isnotnull(features#1014)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "            +- Sort [COMM#879 ASC NULLS FIRST], false, 0\n",
      "               +- Filter (isnotnull(total_population#535L) AND (total_population#535L > 0))\n",
      "                  +- HashAggregate(keys=[COMM#879], functions=[sum(POP_2010#888L)], schema specialized)\n",
      "                     +- Exchange hashpartitioning(COMM#879, 200), ENSURE_REQUIREMENTS, [plan_id=2691]\n",
      "                        +- HashAggregate(keys=[COMM#879], functions=[partial_sum(POP_2010#888L)], schema specialized)\n",
      "                           +- Project [features#33.properties.COMM AS COMM#879, features#33.properties.POP_2010 AS POP_2010#888L]\n",
      "                              +- Filter ((isnotnull(features#33.properties.CITY) AND (features#33.properties.CITY = Los Angeles)) AND isnotnull(features#33.properties.COMM))\n",
      "                                 +- Generate explode(features#869), false, [features#33]\n",
      "                                    +- Filter ((size(features#869, true) > 0) AND isnotnull(features#869))\n",
      "                                       +- FileScan geojson [features#869] Batched: false, DataFilters: [(size(features#869, true) > 0), isnotnull(features#869)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "\n",
      "Total time: 93.6082 seconds\n",
      "+-----------------------+-------------+------------------------+-----------+-----------------------+\n",
      "|COMM                   |Total Income |Median Income Per Person|crime_count|Crimes Per Person Ratio|\n",
      "+-----------------------+-------------+------------------------+-----------+-----------------------+\n",
      "|Adams-Normandie        |6.8942616E7  |8791.4583               |5779       |0.73693                |\n",
      "|Alsace                 |1.3181687E8  |11239.50119             |6466       |0.55133                |\n",
      "|Angeles National Forest|562353.0     |28117.65                |227        |11.35                  |\n",
      "|Angelino Heights       |4.374584E7   |18411.54882             |1497       |0.63005                |\n",
      "|Arleta                 |3.98153976E8 |12110.77917             |14542      |0.44233                |\n",
      "|Atwater Village        |4.0155696E8  |28477.19736             |9696       |0.68761                |\n",
      "|Baldwin Hills          |4.95497352E8 |17302.69763             |34194      |1.19405                |\n",
      "|Bel Air                |5.20784494E8 |63041.33809             |3532       |0.42755                |\n",
      "|Beverly Crest          |7.43010853E8 |60947.4902              |4527       |0.37134                |\n",
      "|Beverlywood            |3.63359998E8 |29267.82102             |6471       |0.52122                |\n",
      "|Boyle Heights          |6.96147034E8 |8425.1762               |59656      |0.72199                |\n",
      "|Brentwood              |1.782691149E9|60840.62486             |14756      |0.5036                 |\n",
      "|Brookside              |1.1264087E7  |18138.62641             |550        |0.88567                |\n",
      "|Cadillac-Corning       |1.3045261E8  |19572.7847              |4445       |0.66692                |\n",
      "|Canoga Park            |1.158639981E9|19656.9564              |52816      |0.89605                |\n",
      "|Carthay                |6.56158931E8 |49841.16453             |12273      |0.93224                |\n",
      "|Central                |2.46980548E8 |6972.51843              |24981      |0.70524                |\n",
      "|Century City           |5.48170741E8 |46103.5106              |10176      |0.85585                |\n",
      "|Century Palms/Cove     |2.6248474E8  |8552.22012              |38877      |1.26668                |\n",
      "|Chatsworth             |1.059535629E9|30580.87653             |23963      |0.69163                |\n",
      "+-----------------------+-------------+------------------------+-----------+-----------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "# Join the two DataFrames on the \"COMM\" column\n",
    "final_combined_result = final_result.hint(\"MERGE\").join(final_result2, on=\"COMM\", how=\"inner\")\n",
    "\n",
    "# Show the execution plan\n",
    "print(\"Execution plan for Final Result:\")\n",
    "final_combined_result.explain()\n",
    "\n",
    "# Add a new column to calculate the ratio of crimes per person\n",
    "final_combined_result = final_combined_result.select(\n",
    "    \"COMM\",\n",
    "    round(col(\"Total Income\"), 5).alias(\"Total Income\"),\n",
    "    round(col(\"Median Income Per Person\"), 5).alias(\"Median Income Per Person\"),\n",
    "    \"crime_count\",\n",
    "    round((col(\"crime_count\") / col(\"total_population\")), 5).alias(\"Crimes Per Person Ratio\"))\n",
    "\n",
    "\n",
    "# Print Total Time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Total time: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "final_combined_result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd65e5b9",
   "metadata": {},
   "source": [
    "## Save final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f22a83e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Optional: Save the final DataFrame as a CSV file for further analysis\n",
    "output_path = \"s3://groups-bucket-dblab-905418150721/group7/q3_results\"\n",
    "final_combined_result.write \\\n",
    "    .option(\"header\", True) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(f\"{output_path}/final_combined_results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
