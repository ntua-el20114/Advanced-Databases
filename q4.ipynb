{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3225e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2562</td><td>application_1732639283265_2521</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2521/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-227.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2521_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing configuration: 4 cores, 8g memory\n",
      "Execution time: 38.301408767700195 seconds\n",
      "Final Top 3 COMM Racial Profile\n",
      "+------------------------------+-------------+-------------------+-------------------+----------------+\n",
      "|vict_descent                  |total_victims|comm1              |comm2              |comm3           |\n",
      "+------------------------------+-------------+-------------------+-------------------+----------------+\n",
      "|White                         |8429         |Pacific Palisades  |Palisades Highlands|Marina Peninsula|\n",
      "|Other                         |1125         |Palisades Highlands|Pacific Palisades  |Marina Peninsula|\n",
      "|Hispanic/Latin/Mexican        |868          |Palisades Highlands|Pacific Palisades  |Marina Peninsula|\n",
      "|Unknown                       |651          |Pacific Palisades  |Palisades Highlands|Marina Peninsula|\n",
      "|Black                         |462          |Pacific Palisades  |Palisades Highlands|Marina Peninsula|\n",
      "|Other Asian                   |314          |Palisades Highlands|Pacific Palisades  |Marina Peninsula|\n",
      "|Chinese                       |42           |Pacific Palisades  |Palisades Highlands|Marina Peninsula|\n",
      "|Japanese                      |20           |Pacific Palisades  |N/A                |N/A             |\n",
      "|Filipino                      |18           |Pacific Palisades  |Palisades Highlands|N/A             |\n",
      "|Korean                        |18           |Pacific Palisades  |Marina Peninsula   |N/A             |\n",
      "|American Indian/Alaskan Native|7            |Palisades Highlands|Pacific Palisades  |Marina Peninsula|\n",
      "|AsianIndian                   |6            |Pacific Palisades  |Marina Peninsula   |N/A             |\n",
      "|Vietnamese                    |5            |Pacific Palisades  |Marina Peninsula   |N/A             |\n",
      "|Hawaiian                      |2            |Pacific Palisades  |Marina Peninsula   |N/A             |\n",
      "+------------------------------+-------------+-------------------+-------------------+----------------+\n",
      "\n",
      "Final Bottom 3 COMM Racial Profile\n",
      "+------------------------------+-------------+---------------+---------------+--------------+\n",
      "|vict_descent                  |total_victims|comm1          |comm2          |comm3         |\n",
      "+------------------------------+-------------+---------------+---------------+--------------+\n",
      "|Hispanic/Latin/Mexican        |47026        |South Park     |University Park|Vernon Central|\n",
      "|Black                         |17151        |South Park     |University Park|Vernon Central|\n",
      "|White                         |7265         |South Park     |University Park|Vernon Central|\n",
      "|Other                         |3256         |South Park     |University Park|Vernon Central|\n",
      "|Unknown                       |2865         |South Park     |University Park|Vernon Central|\n",
      "|Other Asian                   |1979         |South Park     |University Park|Vernon Central|\n",
      "|American Indian/Alaskan Native|299          |University Park|South Park     |Vernon Central|\n",
      "|Chinese                       |145          |South Park     |University Park|Vernon Central|\n",
      "|Korean                        |103          |South Park     |University Park|Vernon Central|\n",
      "|Filipino                      |56           |South Park     |University Park|Vernon Central|\n",
      "|Japanese                      |27           |South Park     |University Park|Vernon Central|\n",
      "|Vietnamese                    |23           |South Park     |University Park|Vernon Central|\n",
      "|AsianIndian                   |22           |South Park     |University Park|N/A           |\n",
      "|Hawaiian                      |14           |University Park|South Park     |Vernon Central|\n",
      "|Pacific Islander              |9            |South Park     |University Park|Vernon Central|\n",
      "|Cambodian                     |8            |South Park     |University Park|Vernon Central|\n",
      "|Guamanian                     |7            |South Park     |University Park|Vernon Central|\n",
      "|Laotian                       |7            |University Park|South Park     |Vernon Central|\n",
      "|Samoan                        |2            |South Park     |N/A            |N/A           |\n",
      "+------------------------------+-------------+---------------+---------------+--------------+\n",
      "\n",
      "Configuration 4 cores, 8g memory executed in 38.301408767700195 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, sum as _sum, collect_list, coalesce, lit, expr\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Function to create Spark session with specific configuration\n",
    "def create_spark_session(executor_cores, executor_memory):\n",
    "    return SparkSession.builder \\\n",
    "        .appName(f\"Query4 - {executor_cores} cores, {executor_memory} memory\") \\\n",
    "        .config(\"spark.executor.instances\", 2) \\\n",
    "        .config(\"spark.executor.cores\", executor_cores) \\\n",
    "        .config(\"spark.executor.memory\", executor_memory) \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Function to execute the main processing logic\n",
    "def main_processing(spark, config):\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create Sedona Context\n",
    "    sedona = SedonaContext.create(spark)\n",
    "\n",
    "    # Load the census GeoJSON dataset\n",
    "    geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "    blocks_df = sedona.read.format(\"geojson\") \\\n",
    "        .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "        .selectExpr(\"explode(features) as features\") \\\n",
    "        .select(\"features.*\")\n",
    "\n",
    "    # Flatten GeoJSON properties\n",
    "    flattened_df = blocks_df.select(\n",
    "        [col(f\"properties.{col_name}\").alias(col_name) for col_name in\n",
    "         blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]\n",
    "    ).drop(\"properties\").drop(\"type\")\n",
    "\n",
    "    # Filter for Los Angeles City\n",
    "    flattened_df = flattened_df.filter(col(\"CITY\") == \"Los Angeles\")\n",
    "\n",
    "    # Load crime datasets\n",
    "    crime_data_path_1 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "    crime_data_path_2 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "\n",
    "    data_1 = spark.read.csv(crime_data_path_1, header=True, inferSchema=True)\n",
    "    data_2 = spark.read.csv(crime_data_path_2, header=True, inferSchema=True)\n",
    "\n",
    "    # Standardize column names (trim spaces)\n",
    "    data_1 = data_1.select([col(c).alias(c.strip()) for c in data_1.columns])\n",
    "    data_2 = data_2.select([col(c).alias(c.strip()) for c in data_2.columns])\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_crime_df = data_1.unionByName(data_2)\n",
    "\n",
    "    # Remove rows with empty Vict Descent\n",
    "    combined_crime_df = combined_crime_df.filter((col(\"Vict Descent\").isNotNull()) & (col(\"Vict Descent\") != \"\"))\n",
    "\n",
    "    # Create geometry points for crimes\n",
    "    combined_crime_df = combined_crime_df.withColumn(\n",
    "        \"geom\", ST_Point(col(\"LON\"), col(\"LAT\"))\n",
    "    ).dropna(subset=[\"LON\", \"LAT\"])\n",
    "\n",
    "    # Perform spatial join to assign COMM to each crime\n",
    "    crimes_with_comm = combined_crime_df.join(\n",
    "        flattened_df, ST_Within(combined_crime_df.geom, flattened_df.geometry), \"inner\"\n",
    "    ).select(\"COMM\", \"Vict Descent\", \"DR_NO\")\n",
    "\n",
    "    # Load income dataset\n",
    "    income_dataset_path = \"s3://groups-bucket-dblab-905418150721/group7/q3_results/median_income_per_person/part-00000-f13c9655-4e09-47ad-b0e7-080964d8ab97-c000.csv\"\n",
    "    income_df = spark.read.csv(income_dataset_path, header=True, inferSchema=True)\n",
    "\n",
    "    # Get 3 COMM with highest and lowest income\n",
    "    top_3_comm = income_df.orderBy(col(\"Median Income Per Person\").desc()).limit(3).select(\"COMM\")\n",
    "    bottom_3_comm = income_df.orderBy(col(\"Median Income Per Person\").asc()).limit(3).select(\"COMM\")\n",
    "\n",
    "    # Filter crimes for top and bottom 3 COMM\n",
    "    top_3_crimes = crimes_with_comm.join(top_3_comm, \"COMM\", \"inner\")\n",
    "    bottom_3_crimes = crimes_with_comm.join(bottom_3_comm, \"COMM\", \"inner\")\n",
    "\n",
    "    # Load ethnicity codes\n",
    "    ethnicity_codes_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\"\n",
    "    ethnicity_df = spark.read.csv(ethnicity_codes_path, header=True, inferSchema=True)\n",
    "\n",
    "    # Add ethnicity descriptions\n",
    "    top_3_crimes = top_3_crimes.join(ethnicity_df, \"Vict Descent\", \"left\")\n",
    "    bottom_3_crimes = bottom_3_crimes.join(ethnicity_df, \"Vict Descent\", \"left\")\n",
    "\n",
    "    # Aggregate racial profile for top 3 COMM\n",
    "    top_3_racial_profile = top_3_crimes.groupBy(\"COMM\", \"Vict Descent Full\").agg(count(\"DR_NO\").alias(\"victim_count\"))\n",
    "\n",
    "    # Aggregate racial profile for bottom 3 COMM\n",
    "    bottom_3_racial_profile = bottom_3_crimes.groupBy(\"COMM\", \"Vict Descent Full\").agg(count(\"DR_NO\").alias(\"victim_count\"))\n",
    "\n",
    "    # Aggregate total victims per race/ethnicity and gather contributing COMM areas for top 3 COMM\n",
    "    final_top_3_racial_profile = top_3_racial_profile.groupBy(\"Vict Descent Full\") \\\n",
    "        .agg(\n",
    "            _sum(\"victim_count\").alias(\"total_victims\"),\n",
    "            collect_list(\"COMM\").alias(\"comm_contributors\")\n",
    "        ).withColumnRenamed(\"Vict Descent Full\", \"vict_descent\")\n",
    "\n",
    "    # Extract individual COMM counts for top 3\n",
    "    final_top_3_racial_profile = final_top_3_racial_profile.select(\n",
    "        col(\"vict_descent\"),\n",
    "        col(\"total_victims\"),\n",
    "        col(\"comm_contributors\")[0].alias(\"comm1\"),\n",
    "        coalesce(col(\"comm_contributors\")[1], lit(\"N/A\")).alias(\"comm2\"),\n",
    "        coalesce(col(\"comm_contributors\")[2], lit(\"N/A\")).alias(\"comm3\")\n",
    "    )\n",
    "    final_top_3_racial_profile = final_top_3_racial_profile.orderBy(col(\"total_victims\").desc())\n",
    "\n",
    "\n",
    "    # Repeat for bottom 3 COMM\n",
    "    final_bottom_3_racial_profile = bottom_3_racial_profile.groupBy(\"Vict Descent Full\") \\\n",
    "        .agg(\n",
    "            _sum(\"victim_count\").alias(\"total_victims\"),\n",
    "            collect_list(\"COMM\").alias(\"comm_contributors\")\n",
    "        ).withColumnRenamed(\"Vict Descent Full\", \"vict_descent\")\n",
    "\n",
    "    # Extract individual COMM counts for bottom 3\n",
    "    final_bottom_3_racial_profile = final_bottom_3_racial_profile.select(\n",
    "        col(\"vict_descent\"),\n",
    "        col(\"total_victims\"),\n",
    "        col(\"comm_contributors\")[0].alias(\"comm1\"),\n",
    "        coalesce(col(\"comm_contributors\")[1], lit(\"N/A\")).alias(\"comm2\"),\n",
    "        coalesce(col(\"comm_contributors\")[2], lit(\"N/A\")).alias(\"comm3\")\n",
    "    )\n",
    "    final_bottom_3_racial_profile = final_bottom_3_racial_profile.orderBy(col(\"total_victims\").desc())\n",
    "\n",
    "    # Save execution time\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time: {execution_time} seconds\")\n",
    "\n",
    "    # Save results to separate paths\n",
    "    output_path_final_top = \"s3://groups-bucket-dblab-905418150721/group7/q4_results/final_top_3_racial_profile\"\n",
    "    output_path_final_bottom = \"s3://groups-bucket-dblab-905418150721/group7/q4_results/final_bottom_3_racial_profile\"\n",
    "\n",
    "    final_top_3_racial_profile.write.option(\"header\", True).mode(\"overwrite\").csv(f\"{output_path_final_top}/{config}\")\n",
    "    final_bottom_3_racial_profile.write.option(\"header\", True).mode(\"overwrite\").csv(f\"{output_path_final_bottom}/{config}\")\n",
    "\n",
    "    # Show results\n",
    "    print(\"Final Top 3 COMM Racial Profile\")\n",
    "    final_top_3_racial_profile.show(truncate=False)\n",
    "\n",
    "    print(\"Final Bottom 3 COMM Racial Profile\")\n",
    "    final_bottom_3_racial_profile.show(truncate=False)\n",
    "    \n",
    "    return execution_time\n",
    "\n",
    "# Test configurations\n",
    "config =  {\"cores\": 4, \"memory\": \"8g\"}\n",
    "    #{\"cores\": 1, \"memory\": \"2g\"}\n",
    "    #{\"cores\": 2, \"memory\": \"4g\"}\n",
    "\n",
    "                                                                                     \n",
    "# Run for each configuration\n",
    "print(f\"Testing configuration: {config['cores']} cores, {config['memory']} memory\")\n",
    "spark = create_spark_session(config['cores'], config['memory'])\n",
    "execution_time = main_processing(spark,f\"{config['cores']}\")\n",
    "print(f\"Configuration {config['cores']} cores, {config['memory']} memory executed in {execution_time} seconds\")\n",
    "spark.stop()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afab124e",
   "metadata": {},
   "source": [
    "Testing configuration: 1 cores, 2g memory\n",
    "Execution time: 86.27664875984192 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addcad0b",
   "metadata": {},
   "source": [
    "Testing configuration: 2 cores, 4g memory\n",
    "Execution time: 25.129631757736206 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f06f4c",
   "metadata": {},
   "source": [
    "Testing configuration: 4 cores, 8g memory\n",
    "Execution time: 38.301408767700195 seconds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
