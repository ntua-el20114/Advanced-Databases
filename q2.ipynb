{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5270f48",
   "metadata": {},
   "source": [
    "# Query 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5366d0",
   "metadata": {},
   "source": [
    "## Import and describe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6064362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2536</td><td>application_1732639283265_2495</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2495/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2495_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import * #col, when, count\n",
    "import time\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query2\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load and filter Data\n",
    "crime_data_path_1 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "crime_data_path_2 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "data_1 = spark.read.csv(crime_data_path_1, header=True, inferSchema=True) \\\n",
    "    .withColumnRenamed('AREA ', 'AREA')\n",
    "data_2 = spark.read.csv(crime_data_path_2, header=True, inferSchema=True)\n",
    "data = (\n",
    "    data_1.union(data_2)\n",
    "    # Keep only selected columns\n",
    "    .select('DR_NO', 'DATE OCC', 'AREA', 'AREA NAME', 'Status', 'Status Desc')\n",
    "    # Convert DATE OCC to timestamp type\n",
    "    .withColumn(\"DATE OCC\", to_timestamp(\"DATE OCC\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb8df2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+----+---------+------+------------+\n",
      "|   DR_NO|           DATE OCC|AREA|AREA NAME|Status| Status Desc|\n",
      "+--------+-------------------+----+---------+------+------------+\n",
      "| 1307355|2010-02-20 00:00:00|  13|   Newton|    AA|Adult Arrest|\n",
      "|11401303|2010-09-12 00:00:00|  14|  Pacific|    IC| Invest Cont|\n",
      "|70309629|2010-08-09 00:00:00|  13|   Newton|    IC| Invest Cont|\n",
      "+--------+-------------------+----+---------+------+------------+\n",
      "\n",
      "+-------+--------------------+-----------------+-----------+-----------------+------------+\n",
      "|summary|               DR_NO|             AREA|  AREA NAME|           Status| Status Desc|\n",
      "+-------+--------------------+-----------------+-----------+-----------------+------------+\n",
      "|  count|             3113337|          3113337|    3113337|          3113333|     3113337|\n",
      "|   mean|1.7099427277162287E8|10.96417927130921|       NULL|             16.0|        NULL|\n",
      "| stddev| 4.182187908303369E7|6.046025657331321|       NULL|4.242640687119285|        NULL|\n",
      "|    min|                 817|                1|77th Street|               13|Adult Arrest|\n",
      "|    max|           910220366|               21|   Wilshire|               TH|         UNK|\n",
      "+-------+--------------------+-----------------+-----------+-----------------+------------+\n",
      "\n",
      "root\n",
      " |-- DR_NO: integer (nullable = true)\n",
      " |-- DATE OCC: timestamp (nullable = true)\n",
      " |-- AREA: integer (nullable = true)\n",
      " |-- AREA NAME: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Status Desc: string (nullable = true)\n",
      "\n",
      "+------+------------+\n",
      "|Status| Status Desc|\n",
      "+------+------------+\n",
      "|    JA|  Juv Arrest|\n",
      "|    AA|Adult Arrest|\n",
      "|    JO|   Juv Other|\n",
      "|    AO| Adult Other|\n",
      "|    IC| Invest Cont|\n",
      "|    13|         UNK|\n",
      "|    CC|         UNK|\n",
      "|  NULL|         UNK|\n",
      "|    19|         UNK|\n",
      "|    TH|         UNK|\n",
      "+------+------------+\n",
      "\n",
      "+----+-----------+\n",
      "|AREA|  AREA NAME|\n",
      "+----+-----------+\n",
      "|   1|    Central|\n",
      "|   2|    Rampart|\n",
      "|   3|  Southwest|\n",
      "|   4| Hollenbeck|\n",
      "|   5|     Harbor|\n",
      "|   6|  Hollywood|\n",
      "|   7|   Wilshire|\n",
      "|   8|    West LA|\n",
      "|   9|   Van Nuys|\n",
      "|  10|West Valley|\n",
      "|  11|  Northeast|\n",
      "|  12|77th Street|\n",
      "|  13|     Newton|\n",
      "|  14|    Pacific|\n",
      "|  15|N Hollywood|\n",
      "|  16|   Foothill|\n",
      "|  17| Devonshire|\n",
      "|  18|  Southeast|\n",
      "|  19|    Mission|\n",
      "|  20|    Olympic|\n",
      "+----+-----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "# View first lines of dataset (for validation)\n",
    "data.limit(3).show()\n",
    "\n",
    "# Describe the dataset\n",
    "data.describe().show() \n",
    "\n",
    "# Print schema\n",
    "data.printSchema()\n",
    "\n",
    "# Get distinct values of some columns\n",
    "data.select('Status', 'Status Desc').distinct().show()\n",
    "data.select('AREA', 'AREA NAME').distinct().orderBy('AREA').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c7838b",
   "metadata": {},
   "source": [
    "## Query - DataFrames API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d646f4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------------+----+\n",
      "|year|  AREA NAME|       closed_rate|rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|    Rampart| 32.84713448949121|   1|\n",
      "|2010|    Olympic|31.515289821999087|   2|\n",
      "|2010|     Harbor| 29.36028339237341|   3|\n",
      "|2011|    Olympic|35.040060090135206|   1|\n",
      "|2011|    Rampart|  32.4964471814306|   2|\n",
      "|2011|     Harbor| 28.51336246316431|   3|\n",
      "|2012|    Olympic| 34.29708533302119|   1|\n",
      "|2012|    Rampart| 32.46000463714352|   2|\n",
      "|2012|     Harbor|29.509585848956675|   3|\n",
      "|2013|    Olympic| 33.58217940999398|   1|\n",
      "|2013|    Rampart|  32.1060382916053|   2|\n",
      "|2013|     Harbor|29.735499940695053|   3|\n",
      "|2014|   Van Nuys|  32.0215235281705|   1|\n",
      "|2014|West Valley| 31.49754809505847|   2|\n",
      "|2014|    Mission|31.224939855653567|   3|\n",
      "|2015|   Van Nuys|32.265140677157845|   1|\n",
      "|2015|    Mission|30.463762673676303|   2|\n",
      "|2015|   Foothill|30.353001803658852|   3|\n",
      "|2016|   Van Nuys|32.194518462124094|   1|\n",
      "|2016|West Valley| 31.40146437042384|   2|\n",
      "+----+-----------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken: 5.04 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load Data and filter data\n",
    "crime_data_path_1 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "crime_data_path_2 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "data_1 = spark.read.csv(crime_data_path_1, header=True, inferSchema=True) \\\n",
    "    .withColumnRenamed('AREA ', 'AREA')\n",
    "data_2 = spark.read.csv(crime_data_path_2, header=True, inferSchema=True)\n",
    "data = (\n",
    "    data_1.union(data_2)\n",
    "    # Drop null island (0,0) entries\n",
    "#     .filter((col('LON') != 0) | (col('LAT') != 0))\n",
    "    # Keep only selected columns\n",
    "    .select('DR_NO', 'DATE OCC', 'AREA', 'AREA NAME', 'Status', 'Status Desc')\n",
    "    # Convert DATE OCC to timestamp type\n",
    "    .withColumn(\"DATE OCC\", to_timestamp(\"DATE OCC\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    ")\n",
    "\n",
    "# Add a \"year\" column\n",
    "data = data.withColumn(\"year\", year(\"DATE OCC\"))\n",
    "\n",
    "# Create a column indicating whether the case is open or closed\n",
    "data = data.withColumn(\"open_closed\", when(col(\"Status\").isin(\"NULL\", \"IC\"), \"open\").otherwise(\"closed\"))\n",
    "\n",
    "# Group by year and police station, and calculate closed cases and total cases\n",
    "cases_grouped = data.groupBy(\"year\", \"AREA NAME\").agg(  \n",
    "    count(\"*\").alias(\"total_cases\"),\n",
    "    sum(when(col(\"open_closed\") == \"closed\", 1).otherwise(0)).alias(\"closed_cases\")\n",
    ")\n",
    "\n",
    "# Calculate the closed cases rate\n",
    "cases_grouped = cases_grouped.withColumn(\n",
    "    \"closed_rate\", (col(\"closed_cases\") / col(\"total_cases\"))*100\n",
    ")\n",
    "\n",
    "# Rank the stations by closed cases rate within each year\n",
    "window_spec = Window.partitionBy(\"year\").orderBy(col(\"closed_rate\").desc())\n",
    "\n",
    "# Add a rank column\n",
    "ranked_stations = cases_grouped.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to get the top 3 stations per year\n",
    "top_3_stations_per_year = ranked_stations.filter(col(\"rank\") <= 3)\n",
    "\n",
    "# Show the result\n",
    "result = top_3_stations_per_year.select(\"year\", \"AREA NAME\", \"closed_rate\", \"rank\")\n",
    "result.show()\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Save Results\n",
    "output_path = \"s3://groups-bucket-dblab-905418150721/group7/q2_results\"\n",
    "result.write \\\n",
    "    .option(\"header\", True) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(f\"{output_path}/dataframes_csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4200b80",
   "metadata": {},
   "source": [
    "## Query - SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4621c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------------+----+\n",
      "|year|  AREA NAME|       closed_rate|rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|    Rampart| 32.84713448949121|   1|\n",
      "|2010|    Olympic|31.515289821999087|   2|\n",
      "|2010|     Harbor| 29.36028339237341|   3|\n",
      "|2011|    Olympic|  35.0400600901352|   1|\n",
      "|2011|    Rampart|32.496447181430604|   2|\n",
      "|2011|     Harbor|28.513362463164313|   3|\n",
      "|2012|    Olympic| 34.29708533302119|   1|\n",
      "|2012|    Rampart| 32.46000463714352|   2|\n",
      "|2012|     Harbor| 29.50958584895668|   3|\n",
      "|2013|    Olympic| 33.58217940999398|   1|\n",
      "|2013|    Rampart|  32.1060382916053|   2|\n",
      "|2013|     Harbor|29.735499940695053|   3|\n",
      "|2014|   Van Nuys|  32.0215235281705|   1|\n",
      "|2014|West Valley| 31.49754809505847|   2|\n",
      "|2014|    Mission| 31.22493985565357|   3|\n",
      "|2015|   Van Nuys|32.265140677157845|   1|\n",
      "|2015|    Mission|30.463762673676303|   2|\n",
      "|2015|   Foothill|30.353001803658852|   3|\n",
      "|2016|   Van Nuys|32.194518462124094|   1|\n",
      "|2016|West Valley| 31.40146437042384|   2|\n",
      "+----+-----------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken: 3.73 seconds"
     ]
    }
   ],
   "source": [
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load Data and modify data\n",
    "crime_data_path_1 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "crime_data_path_2 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "\n",
    "data_1 = spark.read.csv(crime_data_path_1, header=True, inferSchema=True) \\\n",
    "    .withColumnRenamed('AREA ', 'AREA')\n",
    "data_2 = spark.read.csv(crime_data_path_2, header=True, inferSchema=True)\n",
    "\n",
    "data_1.createOrReplaceTempView(\"data_1\")\n",
    "data_2.createOrReplaceTempView(\"data_2\")\n",
    "data = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DR_NO, \n",
    "        TO_TIMESTAMP(`DATE OCC`, 'MM/dd/yyyy hh:mm:ss a') AS `DATE OCC`, \n",
    "        AREA, \n",
    "        `AREA NAME`, \n",
    "        Status, \n",
    "        `Status Desc`\n",
    "    FROM (\n",
    "        SELECT * FROM data_1\n",
    "        UNION ALL\n",
    "        SELECT * FROM data_2\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Create a temporary view for SQL queries\n",
    "data.createOrReplaceTempView(\"crime_data\")\n",
    "\n",
    "# SQL Query to add \"year\" column and determine \"open_closed\" status\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        *, \n",
    "        YEAR(`DATE OCC`) AS year,\n",
    "        CASE \n",
    "            WHEN Status IN ('NULL', 'IC') THEN 'open' \n",
    "            ELSE 'closed' \n",
    "        END AS open_closed\n",
    "    FROM crime_data\n",
    "\"\"\"\n",
    "data_with_columns = spark.sql(query)\n",
    "data_with_columns.createOrReplaceTempView(\"crime_data_with_columns\")\n",
    "\n",
    "# SQL Query to group by year and police station, calculate closed cases and total cases\n",
    "group_query = \"\"\"\n",
    "    SELECT \n",
    "        year, \n",
    "        `AREA NAME`,\n",
    "        COUNT(*) AS total_cases,\n",
    "        SUM(CASE WHEN open_closed = 'closed' THEN 1 ELSE 0 END) AS closed_cases,\n",
    "        100 * SUM(CASE WHEN open_closed = 'closed' THEN 1 ELSE 0 END) / COUNT(*) AS closed_rate\n",
    "    FROM crime_data_with_columns\n",
    "    GROUP BY year, `AREA NAME`\n",
    "\"\"\"\n",
    "cases_grouped = spark.sql(group_query)\n",
    "cases_grouped.createOrReplaceTempView(\"cases_grouped\")\n",
    "\n",
    "# SQL Query to rank stations by closed rate within each year\n",
    "rank_query = \"\"\"\n",
    "    SELECT \n",
    "        *,\n",
    "        ROW_NUMBER() OVER (PARTITION BY year ORDER BY closed_rate DESC) AS rank\n",
    "    FROM cases_grouped\n",
    "\"\"\"\n",
    "ranked_stations = spark.sql(rank_query)\n",
    "ranked_stations.createOrReplaceTempView(\"ranked_stations\")\n",
    "\n",
    "# SQL Query to filter the top 3 stations per year\n",
    "top_3_query = \"\"\"\n",
    "    SELECT \n",
    "        year, \n",
    "        `AREA NAME`, \n",
    "        closed_rate, \n",
    "        rank\n",
    "    FROM ranked_stations\n",
    "    WHERE rank <= 3\n",
    "\"\"\"\n",
    "top_3_stations_per_year = spark.sql(top_3_query)\n",
    "\n",
    "# Show the result\n",
    "result = top_3_stations_per_year.select(\"year\", \"AREA NAME\", \"closed_rate\", \"rank\")\n",
    "result.show()\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Save Results\n",
    "output_path = \"s3://groups-bucket-dblab-905418150721/group7/q2_results\"\n",
    "result.write \\\n",
    "    .option(\"header\", True) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(f\"{output_path}/sql_csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb4d25e",
   "metadata": {},
   "source": [
    "## Why is SQL faster than DataFrames API?\n",
    "- DataFrames API is more verbose => more operations\n",
    "- Declarative structure of SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea588452",
   "metadata": {},
   "source": [
    "## Convert data to .parquet format\n",
    "And store to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd08180e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv_path_1 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "csv_path_2 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "parquet_path_1 = \"s3://groups-bucket-dblab-905418150721/group7/q2_parquets/data_1/\"\n",
    "parquet_path_2 = \"s3://groups-bucket-dblab-905418150721/group7/q2_parquets/data_2/\"\n",
    "\n",
    "data_1 = spark.read.csv(csv_path_1, header=True, inferSchema=True)\n",
    "data_2 = spark.read.csv(csv_path_2, header=True, inferSchema=True)\n",
    "\n",
    "data_1.write.parquet(parquet_path_1, mode=\"overwrite\")\n",
    "data_2.write.parquet(parquet_path_2, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f01feb",
   "metadata": {},
   "source": [
    "## Test SQL+parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "febb63e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------------+----+\n",
      "|year|  AREA NAME|       closed_rate|rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|    Rampart| 32.84713448949121|   1|\n",
      "|2010|    Olympic|31.515289821999087|   2|\n",
      "|2010|     Harbor| 29.36028339237341|   3|\n",
      "|2011|    Olympic|  35.0400600901352|   1|\n",
      "|2011|    Rampart|32.496447181430604|   2|\n",
      "|2011|     Harbor|28.513362463164313|   3|\n",
      "|2012|    Olympic| 34.29708533302119|   1|\n",
      "|2012|    Rampart| 32.46000463714352|   2|\n",
      "|2012|     Harbor| 29.50958584895668|   3|\n",
      "|2013|    Olympic| 33.58217940999398|   1|\n",
      "|2013|    Rampart|  32.1060382916053|   2|\n",
      "|2013|     Harbor|29.735499940695053|   3|\n",
      "|2014|   Van Nuys|  32.0215235281705|   1|\n",
      "|2014|West Valley| 31.49754809505847|   2|\n",
      "|2014|    Mission| 31.22493985565357|   3|\n",
      "|2015|   Van Nuys|32.265140677157845|   1|\n",
      "|2015|    Mission|30.463762673676303|   2|\n",
      "|2015|   Foothill|30.353001803658852|   3|\n",
      "|2016|   Van Nuys|32.194518462124094|   1|\n",
      "|2016|West Valley| 31.40146437042384|   2|\n",
      "+----+-----------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken: 2.95 seconds"
     ]
    }
   ],
   "source": [
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load Data and modify data\n",
    "crime_data_path_1 = \"s3://groups-bucket-dblab-905418150721/group7/q2_parquets/data_1/\"\n",
    "crime_data_path_2 = \"s3://groups-bucket-dblab-905418150721/group7/q2_parquets/data_2/\"\n",
    "\n",
    "data_1 = spark.read.parquet(crime_data_path_1, header=True, inferSchema=True) \\\n",
    "    .withColumnRenamed('AREA ', 'AREA')\n",
    "data_2 = spark.read.parquet(crime_data_path_2, header=True, inferSchema=True)\n",
    "\n",
    "data_1.createOrReplaceTempView(\"data_1\")\n",
    "data_2.createOrReplaceTempView(\"data_2\")\n",
    "data = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DR_NO, \n",
    "        TO_TIMESTAMP(`DATE OCC`, 'MM/dd/yyyy hh:mm:ss a') AS `DATE OCC`, \n",
    "        AREA, \n",
    "        `AREA NAME`, \n",
    "        Status, \n",
    "        `Status Desc`\n",
    "    FROM (\n",
    "        SELECT * FROM data_1\n",
    "        UNION ALL\n",
    "        SELECT * FROM data_2\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Create a temporary view for SQL queries\n",
    "data.createOrReplaceTempView(\"crime_data\")\n",
    "\n",
    "# SQL Query to add \"year\" column and determine \"open_closed\" status\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        *, \n",
    "        YEAR(`DATE OCC`) AS year,\n",
    "        CASE \n",
    "            WHEN Status IN ('NULL', 'IC') THEN 'open' \n",
    "            ELSE 'closed' \n",
    "        END AS open_closed\n",
    "    FROM crime_data\n",
    "\"\"\"\n",
    "data_with_columns = spark.sql(query)\n",
    "data_with_columns.createOrReplaceTempView(\"crime_data_with_columns\")\n",
    "\n",
    "# SQL Query to group by year and police station, calculate closed cases and total cases\n",
    "group_query = \"\"\"\n",
    "    SELECT \n",
    "        year, \n",
    "        `AREA NAME`,\n",
    "        COUNT(*) AS total_cases,\n",
    "        SUM(CASE WHEN open_closed = 'closed' THEN 1 ELSE 0 END) AS closed_cases,\n",
    "        100 * SUM(CASE WHEN open_closed = 'closed' THEN 1 ELSE 0 END) / COUNT(*) AS closed_rate\n",
    "    FROM crime_data_with_columns\n",
    "    GROUP BY year, `AREA NAME`\n",
    "\"\"\"\n",
    "cases_grouped = spark.sql(group_query)\n",
    "cases_grouped.createOrReplaceTempView(\"cases_grouped\")\n",
    "\n",
    "# SQL Query to rank stations by closed rate within each year\n",
    "rank_query = \"\"\"\n",
    "    SELECT \n",
    "        *,\n",
    "        ROW_NUMBER() OVER (PARTITION BY year ORDER BY closed_rate DESC) AS rank\n",
    "    FROM cases_grouped\n",
    "\"\"\"\n",
    "ranked_stations = spark.sql(rank_query)\n",
    "ranked_stations.createOrReplaceTempView(\"ranked_stations\")\n",
    "\n",
    "# SQL Query to filter the top 3 stations per year\n",
    "top_3_query = \"\"\"\n",
    "    SELECT \n",
    "        year, \n",
    "        `AREA NAME`, \n",
    "        closed_rate, \n",
    "        rank\n",
    "    FROM ranked_stations\n",
    "    WHERE rank <= 3\n",
    "\"\"\"\n",
    "top_3_stations_per_year = spark.sql(top_3_query)\n",
    "\n",
    "# Show the result\n",
    "result = top_3_stations_per_year.select(\"year\", \"AREA NAME\", \"closed_rate\", \"rank\")\n",
    "result.show()\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Save Results\n",
    "output_path = \"s3://groups-bucket-dblab-905418150721/group7/q2_results\"\n",
    "result.write \\\n",
    "    .option(\"header\", True) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(f\"{output_path}/sql_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6f1c8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
